{
 "metadata": {
  "name": "",
  "signature": "sha256:e3d90e376b4696cb2b5b56f026384680166dea482f7f1448c32a23a81a7020d2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from BeautifulSoup import BeautifulSoup\n",
      "import requests\n",
      "import urlparse\n",
      "import re\n",
      "\n",
      "def run(url, fd, domains):\n",
      "    p = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
      "    cnt=0\n",
      "    links=set()\n",
      "    glinks=set()\n",
      "    doms=set()\n",
      "    r=requests.get(url, verify=False)\n",
      "    soup = BeautifulSoup(r.text)\n",
      "    if 'google.com/search' in url:\n",
      "        #print soup\n",
      "        for l in soup.findAll('li', {'class': 'g'}):\n",
      "            a = l.find('a')\n",
      "            if 'http' in a['href'].replace('/url?q=','').replace('&sa','')[0]:\n",
      "                links.add(a['href'].replace('/url?q=','').replace('&sa','')[0].encode('utf-8'))\n",
      "    else:\n",
      "        #print \"got other site\"\n",
      "        content = r.text\n",
      "        #print content\n",
      "        for link in p.findall(content):\n",
      "            soup = BeautifulSoup(link)\n",
      "            #print soup.text.split('/')[2]\n",
      "            if '.onion' in soup.text.split('/')[2]:\n",
      "                #domain\n",
      "                d = soup.text.split('/')[1]+soup.text.split('/')[2]\n",
      "                d = d.split(':')[0].replace('.cab','').replace(\"'>http\",'').replace(\".city\",'')\n",
      "                d = d.replace(\".to\",\"\").replace(\".lu\",\"\").replace(\".l\",\"\").replace('www.','').replace('.com','')\n",
      "                if d.endswith('.onion') and d not in domains and d not in doms:\n",
      "                    print d\n",
      "                    doms.add(d)\n",
      "                    fd.write(d+'\\n')\n",
      "                    cnt+=1\n",
      "            else:\n",
      "                #links.add(soup.text)\n",
      "                continue\n",
      "    #add next google page\n",
      "    for i in soup.findAll('a', href=True):\n",
      "        if i['href'].startswith('/search?q=.onion'):\n",
      "            glinks.add('http://www.google.com'+i['href'])\n",
      "    return glinks, links, cnt, doms\n",
      "\n",
      "fd = open('oniondomains2.txt', 'wb')\n",
      "counter = 0\n",
      "domains =set()\n",
      "glinks = set()\n",
      "linkstocheck = set(['http://www.google.com/search?q=.onion&rct=j'])\n",
      "while counter < 5000:\n",
      "    urltocheck = linkstocheck.pop()\n",
      "    print urltocheck\n",
      "    #crawl hits for .onion links\n",
      "    try:\n",
      "        gl,l,c,d=run(urltocheck, fd, domains)\n",
      "        linkstocheck = linkstocheck.union(l)\n",
      "        counter+=c\n",
      "        domains = domains.union(d)\n",
      "        glinks = glinks.union(gl)\n",
      "        print \"length of hit list: \" +str(len(linkstocheck))\n",
      "        print \"Number of onion links: \" +str(counter)\n",
      "    except:\n",
      "        continue\n",
      "    #crawl another google site\n",
      "    urltocheck = glinks.pop()\n",
      "    try:\n",
      "        gl,l,c,d=run(urltocheck, fd, domains)\n",
      "        linkstocheck = linkstocheck.union(l)\n",
      "        counter+=c\n",
      "        domains = domains.union(d)\n",
      "        glinks = glinks.union(gl)\n",
      "        print \"length of hit list: \" +str(len(linkstocheck))\n",
      "        print \"Number of onion links: \" +str(counter)\n",
      "    except:\n",
      "        continue\n",
      "fd.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://www.google.com/search?q=.onion&rct=j\n",
        "length of hit list: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of onion links: 0\n",
        "length of hit list: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of onion links: 0\n"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "'pop from an empty set'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-5793e58389d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mlinkstocheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'http://www.google.com/search?q=.onion&rct=j'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0murltocheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkstocheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0murltocheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#crawl hits for .onion links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'pop from an empty set'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd = open('oniondomainsinmongo.txt', 'wb')\n",
      "p = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
      "doms=set()\n",
      "for page in db.onions.find():\n",
      "    site = page['html']\n",
      "    for link in p.findall(site):\n",
      "            soup = BeautifulSoup(link)\n",
      "            #print soup.text.split('/')[2]\n",
      "            if '.onion' in soup.text.split('/')[2]:\n",
      "                #domain\n",
      "                d = soup.text.split('/')[1]+soup.text.split('/')[2]\n",
      "                d = d.split(':')[0].replace('.cab','').replace(\"'>http\",'').replace(\".city\",'')\n",
      "                d = d.replace(\".to\",\"\").replace(\".lu\",\"\").replace(\".l\",\"\").replace('www.','').replace('.com','')\n",
      "                if d.endswith('.onion') and d not in domains and d not in doms:\n",
      "                    print d\n",
      "                    doms.add(d)\n",
      "                    fd.write(d+'\\n')\n",
      "                    cnt+=1\n",
      "fd.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}